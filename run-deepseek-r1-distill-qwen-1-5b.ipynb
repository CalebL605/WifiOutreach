{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show you how to use the DeepSeek R1 Distill Qwen 1.5b language model on Kaggle using Kaggle Models. DeepSeek is a powerful language model known for its strong performance in coding and reasoning tasks. A distilled model is a smaller, more efficient version of a larger model, making it faster and easier to use. \n",
    "\n",
    "I specifically chose the 1.5 billion parameter model because it doesn't require high computational power and is able to provide us with very fast responses using a card like the P100. We'll see if those responses are satisfactory.\" In this notebook, we'll walk through a basic example of how to query the model. We'll also wrap the entire process into a Python function for easy reuse. By the end of this notebook, you'll have a clear understanding of how to use DeepSeek R1 Distill Qwen 1.5b on Kaggle.\n",
    "\n",
    "Let's dive in and explore the capabilities of this exciting model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T19:36:16.586385Z",
     "iopub.status.busy": "2025-02-18T19:36:16.58619Z",
     "iopub.status.idle": "2025-02-18T19:36:24.720428Z",
     "shell.execute_reply": "2025-02-18T19:36:24.719449Z",
     "shell.execute_reply.started": "2025-02-18T19:36:16.586364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "import time\n",
    "import re\n",
    "\n",
    "%pip install mistletoe\n",
    "import mistletoe\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()  # Clears unused cached memory\n",
    "torch.cuda.ipc_collect()  # Collects unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T19:36:24.722524Z",
     "iopub.status.busy": "2025-02-18T19:36:24.722136Z",
     "iopub.status.idle": "2025-02-18T19:36:24.729206Z",
     "shell.execute_reply": "2025-02-18T19:36:24.728454Z",
     "shell.execute_reply.started": "2025-02-18T19:36:24.722502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "print(f'\\n\\nMemory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model, with its 1.5 billion parameters, is small enough to run efficiently on GPUs like the P100, while still offering promising capabilities. Let's verify that our environment is ready. The GPU has been loaded using CUDA. We can also check the current memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T19:36:24.730745Z",
     "iopub.status.busy": "2025-02-18T19:36:24.730456Z",
     "iopub.status.idle": "2025-02-18T19:37:24.906007Z",
     "shell.execute_reply": "2025-02-18T19:37:24.90534Z",
     "shell.execute_reply.started": "2025-02-18T19:36:24.730716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T19:37:24.907763Z",
     "iopub.status.busy": "2025-02-18T19:37:24.907089Z",
     "iopub.status.idle": "2025-02-18T19:37:24.911764Z",
     "shell.execute_reply": "2025-02-18T19:37:24.91098Z",
     "shell.execute_reply.started": "2025-02-18T19:37:24.907726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "system = \"You are STEM expert.\"\n",
    "prompt = \"A bat and a ball cost €1.10 in total. The bat costs €1.00 more than the ball. How much does the ball cost?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T19:37:24.912788Z",
     "iopub.status.busy": "2025-02-18T19:37:24.91255Z",
     "iopub.status.idle": "2025-02-18T19:38:07.186436Z",
     "shell.execute_reply": "2025-02-18T19:38:07.185783Z",
     "shell.execute_reply.started": "2025-02-18T19:37:24.912768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=3000,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepSeek R1 Distill Qwen 1.5b model is loaded from its Kaggle storage path using AutoModelForCausalLM.from_pretrained(), configured for GPU execution with efficient data typing. Subsequently, the model's tokenizer is loaded to handle text conversion. A system message is crafted, designating the model as a STEM expert, followed by a user prompt presenting a classic bat and ball riddle. These messages are formatted into a conversational structure. The tokenizer then prepares the input text, generating tensors suitable for the model, which are moved to the GPU. The model generates a response, limited to 3000 tokens, using the provided input. The generated tokens are then decoded back into readable text, removing any special tokens. Finally, the decoded response is extracted, ready for display or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T19:38:07.18863Z",
     "iopub.status.busy": "2025-02-18T19:38:07.188399Z",
     "iopub.status.idle": "2025-02-18T19:38:07.198264Z",
     "shell.execute_reply": "2025-02-18T19:38:07.197617Z",
     "shell.execute_reply.started": "2025-02-18T19:38:07.188609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HTML(mistletoe.markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model correctly solved the well-known riddle and it detailed the complete calculations and was able to perform verification. This is definitely the expected result, delivered impressively in just a few seconds using such a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T19:38:07.199438Z",
     "iopub.status.busy": "2025-02-18T19:38:07.199226Z",
     "iopub.status.idle": "2025-02-18T19:38:07.211627Z",
     "shell.execute_reply": "2025-02-18T19:38:07.210841Z",
     "shell.execute_reply.started": "2025-02-18T19:38:07.199419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ask_model(system = \"You are STEM expert.\", prompt = \"Who are you?\"):\n",
    "    start_time = time.time()    \n",
    "    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    print(\"Let me think...\")\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True)\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=3000,\n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    stop_time = time.time()   \n",
    "\n",
    "    #answer_start = f\"\\n\\n**LLM answer**: \\n\\n\" \n",
    "    answer_end = f\"\\n...Answer generation took: {(stop_time - start_time):.1f} seconds\" \n",
    "\n",
    "    return response + answer_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now close all the performed operations into a function that takes two parameters as input: one is the system, which defines the role our language model should adopt, and the other is the prompt - question to language model. Additionally, we'll add a print statement to inform the user that their query is being processed by the language model, and at the end, we'll summarize the total time the operation took. The function's output should be a response in a similar format to the one we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ask_model(prompt = \"What is the smallest positive number that, when divided by 3, 4, and 5, leaves a remainder of 2? Don't use LaTeX symbols, only natural language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T19:49:09.084914Z",
     "iopub.status.busy": "2025-02-18T19:49:09.08458Z",
     "iopub.status.idle": "2025-02-18T19:49:58.289611Z",
     "shell.execute_reply": "2025-02-18T19:49:58.288319Z",
     "shell.execute_reply.started": "2025-02-18T19:49:09.084887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"A man is looking at a portrait. Someone asks him, 'Whose picture are you looking at?'. He replies, 'Brothers and sisters I have none, but that man's father is my father's son.' Whose picture is the man looking at?\",\n",
    "    \"You have two ropes, and each one takes exactly one hour to burn completely. The ropes are not necessarily the same length or width, and they don't burn at a uniform rate. How can you measure exactly 45 minutes using only these two ropes and a lighter?\",\n",
    "    \"What is the next number in the following sequence: 1, 11, 21, 1211, 111221, 312211, ...?\",\n",
    "    \"A lily pad doubles in size every day. If it takes 48 days for the lily pad to cover the entire lake, how long does it take for the lily pad to cover half of the lake?\"\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i in questions:\n",
    "    output = ask_model(prompt = i)\n",
    "    print(f\"Output for {questions.index(i)} iteration generated successfully!\")\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have an example of calling a function in a loop - for a list of questions (in this case 4 consecutive math puzzles) we create a list of answers. This can be useful when we want to batch process.\n",
    "\n",
    "That's all for this short notebook. I hope it has been helpful. I encourage you to fork and use this small model for your own experiments to see if, despite its compact size, it can solve your problems.\n",
    "\n",
    "*The descriptive part and code fragments were generated with the support of Generative Artificial Intelligence (Gemini 2.0 Flash model)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credentials**:\n",
    "1. https://www.kaggle.com/code/saadasif/deepseek-r1-7b-running-on-kaggle-gpu-p100\n",
    "2. https://github.com/speakleash/Bielik-how-to-start/blob/main/Bielik_(4_bit)_Text_Improvement.ipynb\n",
    "3. https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "4. https://www.kaggle.com/models/deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-1.5b/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thanks for reading my notebook!**\n",
    "\n",
    "**If you have any suggestions for improving the analysis, let me know in the comment!**\n",
    "\n",
    "**If you liked my notebook, give upvote!**\n",
    "\n",
    "**If you have a moment, I encourage you to see my other [projects](https://www.kaggle.com/michau96/code).**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 225262,
     "modelInstanceId": 204042,
     "sourceId": 256574,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
